{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Lib.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP8P98e4pS2g7BaY24FN1S6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB9zSiYlEcUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install category_encoders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WaGi-GoDYwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from  datetime import datetime, timedelta\n",
        "import gc\n",
        "import pickle\n",
        "import numpy as np, pandas as pd\n",
        "import lightgbm as lgb\n",
        "\n",
        "import category_encoders as ce\n",
        "# Настройки отображения в pandas\n",
        "pd.options.display.max_columns = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVFagTc-EikL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Memory_reducer:\n",
        "  '''\n",
        "  Reduce RAM usage\n",
        "  '''\n",
        "  def __init__():\n",
        "    pass\n",
        "  def DF_mem_reduce(df):\n",
        "    '''\n",
        "    Reduce memory size of data frame\n",
        "    '''\n",
        "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "    int_columns = df.select_dtypes(include=[\"int\"]).columns\n",
        "    float_columns = df.select_dtypes(include=[\"float\"]).columns\n",
        "    for col in int_columns:\n",
        "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
        "    for col in float_columns:\n",
        "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
        "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "    print(\"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
        "                end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn53RDY7HPay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class My_Reader(Memory_reducer):\n",
        "  def __init__(self, mem_red=True, info=False):\n",
        "    self.mem_red = mem_red\n",
        "    self.info = info\n",
        "  def mem_reduce(df):\n",
        "    Memory_reducer.DF_mem_reduce(df)\n",
        "  def read_file(self, path, params=False):\n",
        "    name = (path.split('\\\\')[-1])\n",
        "    print(f'reading file {name}  ', end='')\n",
        "    if not params:\n",
        "      data = pd.read_csv(path)\n",
        "    else:\n",
        "      data = pd.read_csv(path, **params)\n",
        "    print('OK')\n",
        "    if self.mem_red:\n",
        "      My_Reader.mem_reduce(data)\n",
        "    if self.info:\n",
        "      display(data)\n",
        "    gc.collect()\n",
        "    return data   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlqWbCCpHPd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Fea_Fabric:\n",
        "  ''' Produce some features '''\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def add_lag(self,  col_name, lag, group=False):\n",
        "    ''' Add one lag '''\n",
        "    lag_col = f\"{col_name}_lag_{lag}\"\n",
        "    if group:\n",
        "      if type(group) is list: temp = group.append(col_name) \n",
        "      else: temp = [group, col_name]\n",
        "      self.data[lag_col] = self.data[temp].groupby(group)[col_name].shift(lag)\n",
        "    else:\n",
        "      self.data[lag_col] = self.data[col_name].shift(lag)\n",
        "\n",
        "  def add_lags(self, col_name, lags, group=False,):\n",
        "    ''' Add many lag '''\n",
        "    if type(lags) is list:\n",
        "      for lag in lags:\n",
        "        self.add_lag(col_name, lag, group)\n",
        "    else:\n",
        "      self.add_lag(col_name, lag, group)\n",
        "\n",
        "  def add_rolling_window(self, col_name, win, func, group=False):\n",
        "    ''' Add rolling window '''\n",
        "    new_col = f'r{func}_{col_name}_{win}'\n",
        "    if group:\n",
        "      if type(group) is list: temp = group.append(col_name)\n",
        "      else: temp = [group, col_name]\n",
        "      self.data[new_col] = self.data[temp] \\\n",
        "                               .groupby(group)[col_name] \\\n",
        "                               .transform(lambda x : x.rolling(win).agg(func))\n",
        "    else:\n",
        "      self.data[new_col] = self.data[temp] \\\n",
        "                               .transform(lambda x : x.rolling(win).agg(func))\n",
        "\n",
        "\n",
        "  def add_rolling_windows(self, col_name, wins, func, group=False):\n",
        "    ''' Add many rolling windows '''\n",
        "    for win in wins:\n",
        "      self.add_rolling_window(col_name, win, func, group)\n",
        "  \n",
        "  def add_date_features(self, date_col):\n",
        "    ''' \n",
        "    Convert datetime column into separate year, month,\n",
        "    week of year, quarter, week day, month day features\n",
        "    '''\n",
        "    date_features = {\n",
        "        \"wday\": \"weekday\",\n",
        "        \"week\": \"weekofyear\",\n",
        "        \"month\": \"month\",\n",
        "        \"quarter\": \"quarter\",\n",
        "        \"year\": \"year\",\n",
        "        \"mday\": \"day\",\n",
        "                    }\n",
        "    for date_feat_name, date_feat_func in date_features.items():\n",
        "        if date_feat_name in self.data.columns:\n",
        "            self.data[date_feat_name] = self.data[date_feat_name].astype(\"int16\")\n",
        "        else:\n",
        "            self.data[date_feat_name] = getattr(self.data[date_col].dt, date_feat_func).astype(\"int16\")\n",
        "\n",
        "  def target_encoding(self, cat_col):\n",
        "    ''' Target encoding one or meny columns '''\n",
        "    target_enc = ce.TargetEncoder(cols=cat_col)\n",
        "\n",
        "    # Fit the encoder using the categorical features and target\n",
        "    target_enc.fit(self.train[cat_col], self.train[self.target])\n",
        "\n",
        "    # Transform the features, rename the columns with _target suffix, and join to dataframe\n",
        "    self.train = self.train.join(target_enc.transform(self.train[cat_col]).add_suffix('_target'))\n",
        "    self.valid = self.valid.join(target_enc.transform(self.valid[cat_col]).add_suffix('_target'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzU2psLzHPks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Custom_CV_Splitter:\n",
        "  ''' Create custom splitter for cross validation '''\n",
        "  def __init__(self, data, folds):\n",
        "    self.data = data\n",
        "    self.folds = folds\n",
        "  \n",
        "  def get_slice(self):\n",
        "    pass\n",
        "\n",
        "class Custom_TSS(Custom_CV_Splitter):\n",
        "  ''' Create time series splitter '''\n",
        "  def __init__(self, data, folds):\n",
        "    Custom_CV_Splitter.__init__(self, data, folds)\n",
        "\n",
        "  def make_sample_gen(self):\n",
        "    ''' Make time series split generator '''\n",
        "    for start, split, end in self.folds:\n",
        "      train_index = self.data.loc[(self.data >= start) & (self.data < split)].index\n",
        "      valid_index = self.data.loc[(self.data >= split) & (self.data <= end)].index\n",
        "      yield (list(train_index), list(valid_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFSQQKldMQRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class My_Pipeline(Fea_Fabric):\n",
        "  def __init__(self, seed=42, **kwargs):\n",
        "    np.random.seed(seed)\n",
        "    for key, value in kwargs.items()\n",
        "      self.__dict__[key] = value\n",
        "    \n",
        "  def __repr__(self):\n",
        "    return self.data.__repr__()\n",
        "  \n",
        "  def save_data(self):\n",
        "    F = open('M5_data.pkl', 'wb')\n",
        "    pickle.dump(self.data, F)\n",
        "    F.close\n",
        "  \n",
        "  def continue_work(self):\n",
        "    F = open('M5_data.pkl', 'rb')\n",
        "    self.data = pickle.load(F)\n",
        "\n",
        "\n",
        "  def cat_encoding(data):\n",
        "    ''' Convert cat features into int16  '''\n",
        "    for col in data.columns:\n",
        "      if str(data[col].dtype) == 'category':\n",
        "        data[col] = data[col].cat.codes.astype('int16')\n",
        "        data[col] -= data[col].min()\n",
        "\n",
        "  # def read_data(self, is_train = True, nrows = None, first_day = 1200,):\n",
        "  def read_data(self,):\n",
        "    ''' Read and tansform data into one data frame '''\n",
        "    pass\n",
        "  \n",
        "  def add_features(self, fea_dict):\n",
        "    ''' Add many different features at the sametime '''\n",
        "    for methods, params in fea_dict.items():\n",
        "      Fea_Fabric.__dict__[methods](self, **params)\n",
        "  \n",
        "  def train_val_test_split(self, col_name, split_points):\n",
        "    ''' Split set in train, validation and test samples ''' \n",
        "    self.train = self.data.loc[self.data[col_name] <= split_points[0]]\n",
        "    self.valid = self.data.loc[(self.data[col_name] > split_points[0]) & (self.data[col_name] <= split_point[1])]\n",
        "    self.test = self.data.loc[self.data[col_name] > split_points[1]]\n",
        "    \n",
        "    del self.data\n",
        "    gc.collect()\n",
        "\n",
        "  def features_filter(data, useful_fea=None, useless_fea=None):\n",
        "    if useful_fea:\n",
        "      useless_fea = data.columns[~data.columns.isin(useless_fea)]  \n",
        "    data.drop(columns=useless_fea, inplace=True)\n",
        "    gc.collect()\n",
        "\n",
        "  def create_lgb_dataset(self, cat_feas='auto', train=False, valid=False, test=True,\n",
        "                         useful_fea=None, useless_fea=None, drop_nan=True):\n",
        "    ''' Create lgb datasets '''\n",
        "    if drop_nan:\n",
        "      self.train.dropna(inplace=True)\n",
        "    if train:\n",
        "      My_Pypline.features_filter(self.train, useful_fea=useful_fea, useless_fea=useless_fea)\n",
        "      self.train_data = lgb.Dataset(self.train.drop(columns=[self.target]), label = self.train[self.target],\n",
        "                                    categorical_feature=cat_feas,\n",
        "                                    free_raw_data=False)\n",
        "    if valid:\n",
        "      My_Pypline.features_filter(self.train, useful_fea=useful_fea, useless_fea=useless_fea)\n",
        "      self.valid_data = lgb.Dataset(self.valid.drop(columns=[self.target]), label = self.train[self.target],\n",
        "                                    categorical_feature=cat_feas,\n",
        "                                    free_raw_data=False)\n",
        "    \n",
        "    del self.valid, self.train\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "  \n",
        "  def set_CV(self, CV=False, folds=None):\n",
        "    ''' Create TSS generator '''\n",
        "    if CV == 'tss':\n",
        "      tss = Custom_TSS(self.date_list, folds)\n",
        "      self.CV_gen = tss.make_sample_gen()\n",
        "\n",
        "  def fit_model(self, params, CV=False,):\n",
        "    if CV:\n",
        "      self.lgb_model = lgb.train(params, self.train_data, valid_sets = self.valid_data, verbose_eval=20)\n",
        "    else:\n",
        "      self.lgb_model = lgb.train(params, self.train_data, verbose_eval=20)\n",
        "  \n",
        "  def predict(self):\n",
        "    self.test.drop(columns=[self.target])\n",
        "    self.predictions = self.test['id']\n",
        "    self.predictions[self.target] = self.lgb_model.predict(self.test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biIaIYbdMWL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGml-tE9Mkc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5JLSNkiMlaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}